\section{Discussion/Future Work}
As can be seen in Figure 1, our baseline with the image MSE produced encouraging results: the location of the paddle was consistently preserved. However, areas of improvement include the artifacts that exist in this paddle along with the loss of the ball. As seen in Figure 2, use of the latent loss scheme resulted in extreme artifacts, as well as the loss of the paddle and did not result in ball recovery. A new scheme would be needed.\\
Next, our adaptive $\alpha$ scheme was largely ineffective in verifying the expected behavior. Therefore, modifications should be made such that both task accuracy and bitrate should increase inversely with $\alpha$, or a theoretical foundation should found why they do not. This could be possible through other adaptive schemes (thresholds, static ratios, adaptive ratios, etc.). This may also require reflection to analyze if this divergence from expected behavior is driven by mechanisms other than RL loss. \\
On a higher level, our agent suffers from poor generalization. While we cannot yet rule out insufficient training time (due to lack of resources), we believe this comes from lack of information in the encoding. As the encoding is designed to allow the agent to act, it may not be injective. A new architecture is required, for example: using an earlier layer of the feature extractor as the encoding; training the encoder and decoder together, to ensure the encoding retains the correct information; using a different encoder altogether.\\
Furthermore, we would like to explore other environments. Because this environment included a static start position and relatively slow image change, we found that many of the images looked similar (full or almost full block pattern) which lead to poor generalization. Using the agent long enough to train and collect diverse images proved infeasible under time/resource constraints.\\
In addition, a unique issue with train/test separation resulted from the cross-domain approach. Clear separation is an industry standard in the data compression industry. However, RL agents train freely, and therefore it is not possible to make guarantees in advance about which images the agent will train on. Our solution was to generate the test dataset independently from our agent, so that the probability of the same image appearing in both testing and training was low. Evaluation of this probability would need to be done before this could be a solution, but is likely intractable as state probabilities are unequal. A better solution would be to generate a large test set in advance and remove images if they appear during training. It can also be noted that full, final results should be reported on a test set rather than a training set (as done in the Dimension reduction section of experiments). These were done as rudimentary sanity checks, and were included due to lack of better results.\\
Finally, as this paper only touched on the accuracy of the reconstruction, the compression must be analyzed as well.
\\
We also would prefer to do more data preprocessing.
