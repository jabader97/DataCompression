\section{Discussion/Future Work}
As can be seen in Figure 1, our baseline with the image MSE produced encouraging
results: the location of the paddle was consistently preserved. However, areas
of improvement include the artifacts that exist in this paddle along with the
loss of the ball.
%  As seen in Figure 2, use of the latent loss scheme resulted in
% extreme artifacts, as well as the loss of the paddle and did not result in ball
% recovery. A new scheme would be needed.\\
\subsection{Bitrate inconsistency}
When adding the custom loss function to the RL-agent, we saw that we could not verify the desired influence. Increasing $\alpha$ did not necessarily decrease
entropy, but entropy fluctuated strongly between runs. One
reason could be that the RL-agent loss function is quite
unstable in itself. While in some epochs, the agent loss was relatively high, it was fairly small in
other epochs. Therefore, we adjusted $\alpha$
dynamically to achieve to a desired bitrate instead of using a fixed $\alpha$.
However, our adaptive $\alpha$ scheme was largely ineffective in verifying the
expected behavior. Therefore, modifications should be made so both task
accuracy and bitrate increase inversely with $\alpha$, or a theoretical
foundation should found why they do not. This could be possible through other
adaptive schemes (thresholds, static ratios, adaptive ratios, etc.). This may
also require reflection to analyze if this divergence from expected behavior is
driven by mechanisms other than RL loss.
\subsection{Bitrate evaluation}
On the test set, the entropy of the final agent \ref{sub:Results_Compression} is
reasonably small and the image is compressed. However, if in testing we encode
the latents with a Normal distribution as during the training process, the
bitrate would rise. This means that while the agents is able to
compress the data, it is not able to fit the distribution.
Therefore future work needs to either fit the agent better to the data or find
a new method to chose/estimate the latent encoding distribution.

\subsection{Decoder generalization}
On the decoder side, we saw that the image quality decreased in
comparison to the baseline, especially on the test dataset. Although a decrease can
be expected due to compression, the decoder failed to reconstruct important
properties like the paddle position, and just reproduced static properties like
the game frame. While we cannot yet rule out insufficient training time (due to
lack of resources), we believe this comes from lack of information in the
encoding. As the encoding is designed to allow the agent to act, it may not be
injective; hence, many different game states could have the same representation. A
new architecture is required, for example: using an earlier layer of the feature
extractor as the encoding; training the encoder and decoder together; using a different encoder
altogether.

\subsection{Latent loss scheme}
Changing the decoder loss function to our latent loss scheme showed no
improvement. The image becomes noisy and information is distorted. Even
in the lower part of the image where normally the screen is black, the decoder introduces noise. One reason for this behaviour
could be similar to adversarial attacks: the decoder tricks
the encoder into generating the desired latents from totally different input images. One improvement could be to combine the latent loss scheme with $L_2$-Loss
such that $L_2$-Loss preserves the general image quality and penalizes distortion,
and latent loss preserves semantic information.

\subsection{Environment}
Furthermore, we would like to explore other environments. Because this
environment included a static start position and relatively slow image change,
we found that many of the images looked similar (full or almost full block
pattern) which lead to overfitting and consqeuently
poor generalization. Using the agent long enough to train and collect diverse
images proved infeasible under time/resource constraints.

\subsection{Train test separatation}
In addition, a unique issue with train/test separation resulted from the
cross-domain approach. Clear separation is an industry standard for data
compression. However, RL agents train freely, and therefore it is not
possible to make guarantees in advance about training images. Our solution was to generate the test dataset independently from our agent,
so that the probability of the same image appearing in both testing and training
was low. Evaluation of this probability would need to be done before this could
be a solution, but is likely intractable as state probabilities are unequal. A
better solution would be to generate a large test set in advance and remove
images if they appear during training. It can also be noted that full, final
results should be reported on a test set rather than a training set (as done in
the section \ref{sub:Dimension_reduction} of experiments). These were done as rudimentary
sanity checks, and were included due to lack of better results.
\section{Questions}
For further questions about this work (including code, omitted results, etc), please contact the authors at jessica.bader@student.uni-tuebingen.de.