\newcommand{\NUMBER}{8}
\newcommand{\EXERCISES}{2}
\newcommand{\DEADLINE}{6.22.21}
\newcommand{\COURSE}{Data Compression}
\newcommand{\STUDENTA}{Philipp von Bachmann, 4116220}
\newcommand{\STUDENTB}{Jessica Bader, 5624582}
\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath, enumerate, amssymb, multirow, fancyhdr, color, graphicx, lastpage, listings, tikz, pdflscape, subfigure, float, polynom, hyperref, tabularx, forloop, geometry, listings, fancybox, tikz, forest, tabstackengine, cancel}
\input kvmacros
\geometry{a4paper,left=3cm, right=3cm, top=3cm, bottom=3cm}
\pagestyle {fancy}
\fancyhead[C]{\COURSE}
\fancyhead[R]{\today}
\fancyfoot[L]{}
\fancyfoot[C]{}
\fancyfoot[R]{Page \thepage /\pageref*{LastPage}}
\def\header#1#2{
  \begin{center}
    {\Large Progress Update: Jun. 22}\\
    %{(Due by: #2)}
  \end{center}
}

\begin{document}

\begin{tabularx}{\linewidth}{m{0.3 \linewidth}X}
  \begin{minipage}{\linewidth}
    \STUDENTA\\
    \STUDENTB
  \end{minipage}
\end{tabularx}

\section{Introduction and Overview}
    Lossless compression tries to compress data such that the the original data
    can be perfectly reconstructed. However, the
    theoretical limit is given by the data entropy. If one wishes to
    compress further, inevitably some data will be lost. In lossy compression,
    one therefore tries to either minimize the loss for a given bitrate or the
    other way around. But if we lose some data, which data should be lost?
    Traditional loss-functions for image compression, like L2-Loss, give all data
    the same value. However, it seems like some parts of the image retain
    more valuable information than others. Research has tried to
    construct better metrics like MS-SSIM, but finding better metrics is not
    straightforward. This issue is emphasized further in extremely
    lossy compression.

    Going one step back, which data should even be kept in an image? It
    seems reasonable that at least "important" data should be kept. For
    example, if the sky in the background of an image is of slightly different
    colour, then it will not make a difference when detecting a car.
    But if the task is to detect the weather, this difference is suddenly
    significant. Consequently, the task/goals often define the
    importance. We had the idea to define a very general task: keep information
    that enables us to act in an environment. In ML, "acting" and "environment" are commonly dealt with in reinforcement learning.
    In this work, we therefore try to combine RL with
    compression as a way to judge which information should be kept versus dropped, especially in
    very lossy compression.

    Data compression normally consists of three parts: an encoder, a decoder,
    and a evaluation-function. The encoder compresses the information, the
    decoder decompresses, and the evaluation function judges the result. Our goal is to start by training these separately: this way, a single component can be switched out without affecting the others. We will only train these components together if considering them separately does not yield quality results.

    RL agents often have a "feature extractor",
    which extracts relevant information from the input data. If our goal is to
    retain information important for acting, the features extractor
    seems like a natural encoder choice. The decoder, similar to
    normal VAEs, is just a deep NN trying to
    reconstruct the original data. As discussed earlier, a good
    loss-function for the decoder is hard to find. Here we have two
    approaches: on the one hand, using traditional metrics ensures that the
    output image is close to the original, although very rigidly. On the other hand, if our task is to only retain information that
    enables the agent to act, it seems reasonable to let the RL
    agent judge whether it can still act on the reconstructed images. In the end,
    we think trying out both approaches, or using a combination, will yield the
    best results.

\section{Methods}
    \subsection{Encoder/RL agent}
        In our work, we will treat the original RL agent mostly as a black box, as an out of the box
        agent will be flexible enough as a starting point. Additionally, this means
        agents can be swapped out easily and different agent architectures tried. Therefore we will refer to the out-of-the-box loss the agent
        minimizes as $L_{RL}$. But just optimizing this loss
        forgets about our second task: compressing the data. In
        Variational Inference, the compression performance is optimized by
        minimizing $D_{KL} (q_\phi (z\vert x) \Vert p(z))$, where $q_\phi
        (z\vert x)$ is the distribution over the latent space for a given input
        $x$, and $p(z)$ is the distribution over the latents chosen to encode them. Therefore the loss the agent will actually be trained on becomes:
        \begin{equation}
            E_{x \sim env} [L_{RL} + D_{KL} (q_\phi (z\vert x) \Vert p(z))]
        \end{equation}
        % TODO: discuss that x also depends on agent? off-policy vs on-policy

        One problem of NNs for encoding is that they use
        floating point numbers. This results in a very high entropy. One can reduce this entropy by rounding
        the values, for example to the nearest integer. However, this means the
        agent needs to be robust to rounding, otherwise  valuable information could be encoded in small digits
        and consequently destroyed in the rounding process. We try to
        fix this by adding random noise to the latents during
        training, so that the agent becomes more stable against small
        distortions and consequently rounding. This noise is generated as $\epsilon \sim Uni[-\frac{\epsilon_0}{2}, \frac{\epsilon_0}{2}]$ and $\epsilon_0$ is the size of the rounding intervals. During testing, this
        noise will be replaced by rounding.

        Next we need a way to choose $p(z)$ and estimate $q_\phi (z\vert x)$.
        We choose $p(z) \sim \mathcal{N}(0, \sigma_1^2)$, where $\sigma_1^2$ will
        either be fixed as the latent variance of one batch or learned during training. $p(z
        \vert x) \sim \mathcal{N}(\mu , \sigma_2^2)$ where $\mu, \sigma_2^2$ will be
        learned during training.

    \subsection{Decoder}\label{methods:Decoder}
        As mentioned earlier, the decoder will be a normal deep NN. For the decoder loss there are several
        choices: first, we can use L2 loss or variants like MS-SSIM. However, this may be a suboptimal loss. Instead
        also the performance of the agent can be used as a loss. To simplify
        this loss and avoid running the environment during training of the
        decoder (which would be time-costly), we will try to optimize the L2-distance between the latents input for
        the decoder and the latents the original fixed agent generates on the reconstruction of
        the decoder. By the time we train the decoder, the encoder will
        be fixed, therefore the decoder must truly reconstruct (rather than embedding the latent space in a larger image).

    \subsection{Evaluation metric}
        After training, we need a way to evaluate our compression technique.
        Normally this will be done by measuring $L = L_{distortion} + L_{rate}$.
        $L_{rate}$ is given by the latent entropy estimated over
        the test data. $L_{distortion}$ follows as in
        \ref{methods:Decoder}: we can either use a pre-defined Loss-function or evaluate with
        the agent. Here we would use a separately trained agent,
        to eliminate benefiting the encoder by using the same
        agent for evaluation and training. This will also make results
        comparable between runs. In the end, we will report both traditional
        losses and agent loss to show different strengths and weaknesses
        compared to other methods. 
        


\end{document}