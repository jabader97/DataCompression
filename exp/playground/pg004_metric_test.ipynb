{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataCompression.msc.Metric import Metric\n",
    "import numpy as np\n",
    "from DataCompression.src.decoder_MSE import CNN_AE\n",
    "from DataCompression.msc.vae_agarap import AE, CNN_AE_network\n",
    "from DataCompression.src.buffer import Decoder_Buffer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a buffer for images of shape torch.Size([1000, 3, 210, 160]) and latents torch.Size([1000, 128])\n"
     ]
    }
   ],
   "source": [
    "# load the test dataset\n",
    "path = f\"C:/Users/jabad/Documents/Tuebingen/DataCompression/DataCompression/exp/test_dataset\"\n",
    "db = Decoder_Buffer([3, 210, 160], 128, 1000)\n",
    "db.load(path)\n",
    "images = db.images\n",
    "latents = db.latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat the data to fit the decoder method\n",
    "latent = torch.transpose(torch.unsqueeze(torch.unsqueeze(latents, -1), -1).float(), 2, 1)\n",
    "x_train = images.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the model\n",
    "\n",
    "# model = CNN_AE(in_dims=latent[0].shape, out_dims=x_train[0].shape)\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# epochs = 10\n",
    "\n",
    "# # train the model\n",
    "# epoch_losses = []\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = 0\n",
    "#     # reset the gradients to zero\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # compute reconstructions\n",
    "#     outputs = model(latent)\n",
    "#     outputs = torch.squeeze(outputs, 0)\n",
    "\n",
    "#     # compute reconstruction loss\n",
    "#     loss = criterion(outputs, x_train)\n",
    "\n",
    "#     # compute gradients\n",
    "#     loss.backward()\n",
    "\n",
    "#     # perform parameter update\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # add this images's loss to epoch losses (loss per image normalized)\n",
    "#     epoch_losses.append(loss.item() / len(images))\n",
    "\n",
    "#         # display the epoch training loss\n",
    "#     print(\"\\n epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, epoch_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the reconstructed images\n",
    "# outputs = model(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run the metric\n",
    "# metric = Metric(torch.nn.MSELoss())\n",
    "# accuracy, bpd = metric.measure(x_train, outputs, latent)\n",
    "# print(\"MSE Loss:\")\n",
    "# print(accuracy)\n",
    "# print(\"bits/image:\")\n",
    "# print(bpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE Loss:\n",
    "# 3619.858154296875\n",
    "# bits/image:\n",
    "# 0.576"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try for a normal VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original VAE loss:\n",
      "epoch : 1/10, loss = 5767.670846\n",
      "epoch : 2/10, loss = 5713.854795\n",
      "epoch : 3/10, loss = 5663.072440\n",
      "epoch : 4/10, loss = 5613.801942\n",
      "epoch : 5/10, loss = 5565.940284\n",
      "epoch : 6/10, loss = 5519.430361\n",
      "epoch : 7/10, loss = 5474.241786\n",
      "epoch : 8/10, loss = 5430.358787\n",
      "epoch : 9/10, loss = 5387.772857\n",
      "epoch : 10/10, loss = 5346.480477\n"
     ]
    }
   ],
   "source": [
    "# train the original vae to get the low dim spaces (this is slow because used single batches)\n",
    "x_train = images.numpy()\n",
    "print(\"Original VAE loss:\")\n",
    "ae = AE()\n",
    "model = CNN_AE_network(x_train[0].shape, [1, 128, 1])\n",
    "ae.train_ae(x_train, model, epochs=100)\n",
    "# get the lower dimensional space it produced\n",
    "latent = ae.get_latent(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reconstructed images\n",
    "# x_train = torch.unsqueeze(torch.Tensor(x_train), 1)\n",
    "outputs, _ = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:\n",
      "5326.24658203125\n",
      "bits/image:\n",
      "72.192\n"
     ]
    }
   ],
   "source": [
    "# run the metric\n",
    "metric = Metric(torch.nn.MSELoss())\n",
    "latent = np.squeeze(np.asarray(latent), 1)\n",
    "accuracy, bpd = metric.measure(x_train, outputs, latent)\n",
    "print(\"MSE Loss:\")\n",
    "print(accuracy)\n",
    "print(\"bits/image:\")\n",
    "print(bpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b5be0c7d8816b0411dd8381c01b6ac4b538687487d40264816f7c3f94b5666c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
